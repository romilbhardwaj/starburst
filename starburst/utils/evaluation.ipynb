{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "'''\n",
    "sys.path.append('/Users/suryaven/Documents/personal/sky/skyburst')\n",
    "sys.path.append('/Users/suryaven/Documents/personal/sky/starburst/')\n",
    "sys.path.append('/Users/suryaven/Documents/personal/sky/starburst/starburst/utils')\n",
    "'''\n",
    "# TODO: Generalize differences between sky laptop and personal latop paths\n",
    "sys.path.append('/Users/surya/Documents/sky/skyburst/')\n",
    "sys.path.append('/Users/surya/Documents/sky/starburst/')\n",
    "sys.path.append('/Users/surya/Documents/sky/starburst/utils')\n",
    "\n",
    "import starburst\n",
    "import skyburst\n",
    "from starburst import utils\n",
    "import job_logs\n",
    "import sampled_jobs\n",
    "import plot_jobs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time \n",
    "import copy \n",
    "from collections import OrderedDict \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# TODO: Store data for all jobs into one single pandas dataframe\n",
    "# TODO: Store the results of parse_event_logs with associated hyperparameters into single df \n",
    "# TODO: Attach batch hyperparameters associated with each job based on batch it ran on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26235ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS = {\n",
    "    \"05_01_2023\": 1682925843,\n",
    "    \"05_02_2023\": 1683099273,\n",
    "    \"05_03_2023\": 1683132152,\n",
    "    \"05_03_2023_2\": 1683173965,\n",
    "    \"05_04_2023_1\": 1683184792, \n",
    "    \"05_04_2023_2\": 1683196845,\n",
    "    \"05_04_2023_3\": 1683259533,\n",
    "    \"05_05_2023_4\": 1683343491,\n",
    "    \"05_06_2023_4\": 1683410859,\n",
    "    \"05_06_2023_5\": 1683418719,\n",
    "    \"05_06_2023_6\": 1683442154,\n",
    "    \"05_06_2023_7\": 1683452077,\n",
    "    \"05_07_2023_1\": 1683486242,\n",
    "    \"05_07_2023_2\": 1683489564,\n",
    "    \"05_07_2023_3\": 1683496107,\n",
    "    \"05_07_2023_4\": 1683497538,\n",
    "    \"1683498468\": 1683497538, \n",
    "    \"1683498857\": 1683498857,\n",
    "    \"1683499283\": 1683499283,\n",
    "    \"1683528723\": 1683528723, # Fixed JCT values\n",
    "    \"1683534589\": 1683534589, # Large sweep with greater arrival rates\n",
    "}\n",
    "\n",
    "logs = LOGS[\"1683534589\"]\n",
    "\n",
    "job_logs.pull_vm_scheduler_logs(logs)\n",
    "jobs_df = job_logs.retrieve_df(event_number=logs, graph=False, avoid_congestion=False)\n",
    "jobs_df = jobs_df.assign(gpus_per_node=jobs_df['cpus_per_node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609cf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_time(row):\n",
    "    total_time = [row['wait_times'][i] + row['runtime'][i] for i in range(len(row['wait_times']))]\n",
    "    return total_time\n",
    "\n",
    "jobs_df['total_time'] = jobs_df.apply(compute_total_time, axis=1)\n",
    "\n",
    "def compute_completion_time(row):\n",
    "    completion_time = [row['total_time'][i] + row['submission_time'][i] for i in range(len(row['wait_times']))]\n",
    "    return completion_time\n",
    "\n",
    "jobs_df['completion_time'] = jobs_df.apply(compute_completion_time, axis=1)\n",
    "jobs_df['avg_jct'] = jobs_df['total_time'].apply(lambda x: sum(x)/len(x))\n",
    "\n",
    "def compute_cluster_utilization(row):\n",
    "    surface_area = [row['runtime'][i] * row['cpus'][i] for i in range(len(row['runtime']))]\n",
    "    utilized_surface_area = sum(surface_area)\n",
    "    total_surface_area = (max(row['completion_time']) - min(row['submission_time'])) * (row['cpus_per_node'] * row['cluster_size'])\n",
    "    cluster_utilization = utilized_surface_area/total_surface_area\n",
    "    return cluster_utilization\n",
    "\n",
    "jobs_df['cluster_utilization'] = jobs_df.apply(compute_cluster_utilization, axis=1)\n",
    "\n",
    "def compute_system_utilization(row):\n",
    "    # TODO: Compute this value correctly\n",
    "    return system_utilization\n",
    "\n",
    "#jobs_df['cluster_utilization'] = jobs_df.apply(compute_cluster_utilization, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c7931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sum_arrays(row):\n",
    "    new_row = [row['wait_times'][i] + row['runtime'][i] for i in range(len(row['wait_times']))]\n",
    "    return new_row\n",
    "\n",
    "jobs_df['total_time'] = jobs_df.apply(sum_arrays, axis=1)\n",
    "jobs_df['avg_wait'] = jobs_df['wait_times'].apply(lambda x: sum(x)/len(x))\n",
    "jobs_df['avg_runtime'] = jobs_df['runtime'].apply(lambda x: sum(x)/len(x))\n",
    "jobs_df['avg_jct'] = jobs_df['total_time'].apply(lambda x: sum(x)/len(x))\n",
    "#jobs_df\n",
    "\n",
    "jobs_wait = jobs_df.filter(['waiting_policy', 'wait_time', 'wait_times', 'arrival', 'start', 'submission_time', 'arrival_rate'])\n",
    "jobs_wait['arrival_mask'] = jobs_df['start'].apply(lambda x: [0 if not i else 1 for i in x])\n",
    "\n",
    "k8s_scheduling_waiting_constant = 1\n",
    "\n",
    "\n",
    "#print(\"JOB WAIT BEFORE\")\n",
    "#print(jobs_wait['wait_times'])\n",
    "\n",
    "onprem_waits = {}\n",
    "cloud_waits = {}\n",
    "new_waits = {}\n",
    "for j in range(len(jobs_wait)):\n",
    "    onprem_wait = [jobs_wait['wait_times'][j][i] for i, n in enumerate(jobs_wait['arrival_mask'][j]) if n == 1]\n",
    "    #cloud_wait = [jobs_wait['wait_times'][j][i] for i, n in enumerate(jobs_wait['arrival_mask'][j]) if n == 0]\n",
    "    #print(\"CLOUD WAIT BEFORE\")\n",
    "    #print(cloud_wait)\n",
    "    #cloud_wait = [jobs_wait['wait_times'][j][i] + k8s_scheduling_waiting_constant + jobs_wait['wait_time'][j] for i, n in enumerate(jobs_wait['arrival_mask'][j]) if n == 0]\n",
    "    cloud_wait = [k8s_scheduling_waiting_constant + jobs_wait['wait_time'][j] for i, n in enumerate(jobs_wait['arrival_mask'][j]) if n == 0]\n",
    "    #print(\"CLOUD WAIT AFTER\")\n",
    "    #print(cloud_wait)\n",
    "    \n",
    "    #cloud_waits[j] = cloud_wait + jobs_wait['wait_time'] + k8s_scheduling_waiting_constant\n",
    "    \n",
    "    new_wait = onprem_wait + cloud_wait\n",
    "    new_waits[j] = new_wait\n",
    "    onprem_waits[j] = onprem_wait\n",
    "    cloud_waits[j] = cloud_wait\n",
    "    \n",
    "    #new_wait[0] = 0\n",
    "    \n",
    "    jobs_wait['wait_times'][j] = new_wait\n",
    "    #jobs_df['wait_times'][j] = new_wait\n",
    "    #jobs_df.loc[j, 'wait_times'] = new_wait\n",
    "jobs_df['wait_times'] = new_waits\n",
    "\n",
    "    \n",
    "    \n",
    "#print(\"JOB WAIT AFTER\")\n",
    "#print(jobs_wait['wait_times'])\n",
    "\n",
    "#print(onprem_wait)\n",
    "#print(cloud_wait)\n",
    "'''\n",
    "print(\"NEW WAITS\")\n",
    "print(new_waits)\n",
    "print(\"CLOUD WAITS\")\n",
    "print(cloud_waits)\n",
    "print(\"ONPREM WAITS\")\n",
    "print(onprem_waits)\n",
    "'''\n",
    "#time.sleep(100000)\n",
    "#jobs_wait['waitimes']\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(jobs_wait), figsize=(150, 3)) \n",
    "for i in range(len(onprem_waits)):\n",
    "    #TODO: Incorporate tracing for efficiently\n",
    "    #import pdb; pdb.set_trace()\n",
    "    #TODO: Plot separater graphs for onprem and cloud\n",
    "    try: \n",
    "        axs[i].set_title(\"ONPREM \" + \"wait \" + str(jobs_wait['wait_time'][i]) +  \" arr \" + str(jobs_wait['arrival_rate'][i]), fontsize=10)\n",
    "        axs[i].hist(onprem_waits[i])\n",
    "        print(\"Mask\")\n",
    "        print(jobs_wait['arrival_mask'][i])\n",
    "    except Exception as e: \n",
    "        continue \n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(jobs_wait), figsize=(50, 3)) \n",
    "for i in range(len(cloud_waits)):\n",
    "    #TODO: Incorporate tracing for efficiently\n",
    "    #import pdb; pdb.set_trace()\n",
    "    # TODO: Plot separater graphs for onprem and cloud\n",
    "    try: \n",
    "        axs[i].set_title(\"CLOUD \" + \"wait \" + str(jobs_wait['wait_time'][i]) +  \" arr \" + str(jobs_wait['arrival_rate'][i]), fontsize=10)\n",
    "        axs[i].hist(cloud_waits[i])\n",
    "        print(\"Mask\")\n",
    "        print(jobs_wait['arrival_mask'][i])\n",
    "    except Exception as e: \n",
    "        continue \n",
    "plt.show()\n",
    "\n",
    "#time.sleep(100000)\n",
    "'''\n",
    "# TODO: Find avergage of migrated values vs non migrated values\n",
    "\n",
    "waits = {}\n",
    "for i in range(len(jobs_wait)):\n",
    "    try: \n",
    "        onprem_jobs = [jobs_wait['wait_times'][i][j] * jobs_wait['arrival_mask'][i][j] for j in range(len(jobs_wait['wait_times'][i]))]\n",
    "        local_wait = sum(onprem_jobs)/sum(jobs_wait['arrival_mask'][i])\n",
    "\n",
    "        cloud_jobs = [jobs_wait['wait_times'][i][j] * (-1 * (jobs_wait['arrival_mask'][i][j] - 1)) for j in range(len(jobs_wait['wait_times'][i]))]\n",
    "        cloud_wait = sum(cloud_jobs)/(len(cloud_jobs)-sum(jobs_wait['arrival_mask'][i]))\n",
    "\n",
    "        waits[\"wait \" + str(jobs_wait['wait_time'][i]) +  \" arr \" + str(jobs_wait['arrival_rate'][i])] = (local_wait, cloud_wait)\n",
    "\n",
    "        #print(cloud_jobs)\n",
    "        #print(\"wait_times\")\n",
    "        print(jobs_wait['wait_times'][i])\n",
    "        #runtime_diff\n",
    "        #print(\"migrated_times\")\n",
    "        #print(migrated_average)\n",
    "    except Exception as e:\n",
    "        continue \n",
    "\n",
    "#jobs_df_ar_2 = jobs_df.filter()\n",
    "#for i in range(3):\n",
    "    \n",
    "\n",
    "print(\"(local_job_avg_wait, cloud_job_avg_wait)\")\n",
    "for wait in waits: \n",
    "    print((wait, waits[wait]))\n",
    "#print(waits)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(jobs_wait), figsize=(50, 3))  #figsize=(15, 3))\n",
    "\n",
    "for i in range(len(jobs_df)):\n",
    "    #TODO: Incorporate tracing for efficiently\n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    # TODO: Plot separater graphs for onprem and cloud\n",
    "    try: \n",
    "        axs[i].set_title(\"wait \" + str(jobs_wait['wait_time'][i]) +  \" arr \" + str(jobs_wait['arrival_rate'][i]), fontsize=10)\n",
    "        axs[i].hist(jobs_wait['wait_times'][i])\n",
    "        print(\"Mask\")\n",
    "        print(jobs_wait['arrival_mask'][i])\n",
    "    except Exception as e: \n",
    "        continue \n",
    "#print(jobs_df['arrival_mask'][0])\n",
    "#print(jobs_df['arrival_mask'][1])\n",
    "#plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(jobs_wait), figsize=(50, 3))  #figsize=(15, 3))\n",
    "\n",
    "for i in range(len(jobs_df)):\n",
    "    #TODO: Incorporate tracing for efficiently\n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    # TODO: Plot separater graphs for onprem and cloud\n",
    "    try: \n",
    "        axs[i].set_title(\"wait \" + str(jobs_wait['wait_time'][i]) +  \" arr \" + str(jobs_wait['arrival_rate'][i]), fontsize=10)\n",
    "        axs[i].hist(jobs_wait['wait_times'][i])\n",
    "        print(\"Mask\")\n",
    "        print(jobs_wait['arrival_mask'][i])\n",
    "    except Exception as e: \n",
    "        continue \n",
    "plt.show()\n",
    "\n",
    "#jobs_df = all_jobs_original\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55400294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(plot_jobs.GPUS_PER_NODE)\n",
    "#time.sleep(1000)\n",
    "gpus_per_node = 32\n",
    "avoid_congestion = True #False \n",
    "print(gpus_per_node)\n",
    "data = job_logs.view_real_arrival_times(event_number=logs, scale=0.5, plot_sweep=True, get_data=False, dim=(-100, 100, 0, 100), ratio=(5, 75), gpus_per_node = gpus_per_node, avoid_congestion=avoid_congestion)#True)#False)\n",
    "\n",
    "if data and len(data) == 3:\n",
    "    jobs, events, submissions = data\n",
    "    \n",
    "    #print(\"JOBS\")\n",
    "    #print(jobs)\n",
    "    #print(\"EVENTS\")\n",
    "    #print(events)\n",
    "    #print(\"SUBMISSIONS\")\n",
    "    #print(submissions)\n",
    "# TODO: Specify each subplot size\n",
    "# TODO: Specify the yaxis min and yaxis max\n",
    "\n",
    "#job_logs.analyze_sweep(logs, graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa03f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame.from_dict(metrics)\n",
    "metrics_df = metrics_df.transpose()\n",
    "#metrics_df\n",
    "\n",
    "#metrics_df\n",
    "metrics_df.keys()\n",
    "#metrics_df.values()\n",
    "t_metrics_df = metrics_df.transpose()\n",
    "t_metrics_df= t_metrics_df.rename(columns={'cpus_per_node': 'gpus_per_node'})\n",
    "#t_metrics_df= t_metrics_df.rename(columns={'cpus_per_node': 'num_gpus'})\n",
    "t_metrics_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2baf853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_plotting_function():\n",
    "    return 0 \n",
    "\n",
    "def skyburst_plotting_function(): \n",
    "    return 0 \n",
    "\n",
    "def set_plotting_setting(ax):\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    ax.grid(True, which='both')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(True)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    #ax.tick_params(bottom=False, left=False)\n",
    "    ax.tick_params(bottom=True, left=False)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "label_dict = {\n",
    "    'avg_jct': 'Avg. JCT (hr)',\n",
    "    'cost_mult': '% Cost Savings\\nover No Wait',\n",
    "    'cost_diff': 'Cost Savings\\nover No Wait',\n",
    "    'cluster_size': 'Cluster Size (# Nodes)',\n",
    "    'norm_system_utilization': 'System Utilization',\n",
    "    'system_utilization': 'System Utilization',\n",
    "    'cluster_utilization': 'Cluster Utilization',\n",
    "    'total_cloud_cost': 'Cloud Cost',\n",
    "    'arrival_rate': 'Arrival Rate',\n",
    "}\n",
    "\n",
    "legend_dict = {\n",
    "    'constant': 'Constant',\n",
    "    'linear_runtime': 'Runtime',\n",
    "    'linear_cost': 'Cost',\n",
    "    'zero': 'No Wait',\n",
    "    'linear_runtime_filter_cpu': 'Runtime-Preempt-CPU'\n",
    "}\n",
    "\n",
    "def join_baseline(df, \n",
    "                          x_axis: str,\n",
    "                          y_axis: list = ['cost_mult', 'avg_jct'],\n",
    "                          df_filter: dict = {},\n",
    "                          baseline_filter: dict={'waiting_policy': 'zero',},\n",
    "                          groupby_values=['waiting_policy', 'waiting_factor'],\n",
    "                          normalize_x_axis=False):\n",
    "    if isinstance(y_axis, str):\n",
    "        y_axis = [y_axis]\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(y_axis), figsize=(5*len(y_axis), 3.5))\n",
    "    \n",
    "    for k,v in df_filter.items():\n",
    "        df = df[df[k]==v]\n",
    "       \n",
    "    baseline_df = df\n",
    "    for k,v in baseline_filter.items():\n",
    "        assert not isinstance(v, list)\n",
    "        baseline_df = baseline_df[baseline_df[k]==v]\n",
    "        #df = df[df[k]!=v]\n",
    "    diff_df = pd.merge(df, baseline_df, left_on=x_axis,right_on=x_axis)\n",
    "    return diff_df\n",
    "    \n",
    "def get_default(df, \n",
    "                          x_axis: str,\n",
    "                          y_axis: list = ['cost_mult', 'avg_jct'],\n",
    "                          df_filter: dict = {},\n",
    "                          baseline_filter: dict={'waiting_policy': 'zero',},\n",
    "                          groupby_values=['waiting_policy', 'waiting_factor'],\n",
    "                          normalize_x_axis=False):\n",
    "    if isinstance(y_axis, str):\n",
    "        y_axis = [y_axis]\n",
    "        \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(y_axis), figsize=(5*len(y_axis), 3.5))\n",
    "    \n",
    "    for k,v in df_filter.items():\n",
    "        print(k)\n",
    "        print(v)\n",
    "        print(df[k])\n",
    "        #print(df[k]==v)\n",
    "        #df[k] = df[k].apply(lambda x: [0, 0, 0] if len(x) != 3 else x)\n",
    "        #print(df[k].eq(v))\n",
    "        \n",
    "        \n",
    "        mask = df[k].apply(lambda x: x == v)\n",
    "        print(mask)\n",
    "        if isinstance(v, list):\n",
    "            df = df[mask]\n",
    "            #df[k].eq(v)\n",
    "            #df[k] = df[k].apply(lambda x: [0, 0, 0] if len(x) != 3 else x)\n",
    "            #df = df[df[k].eq(v)]\n",
    "            \n",
    "        #df = df[df[k]==v]\n",
    "        \n",
    "    return df\n",
    "    \n",
    "def get_baseline(df, \n",
    "                          x_axis: str,\n",
    "                          y_axis: list = ['cost_mult', 'avg_jct'],\n",
    "                          df_filter: dict = {},\n",
    "                          baseline_filter: dict={'waiting_policy': 'zero',},\n",
    "                          groupby_values=['waiting_policy', 'waiting_factor'],\n",
    "                          normalize_x_axis=False):\n",
    "    if isinstance(y_axis, str):\n",
    "        y_axis = [y_axis]\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(y_axis), figsize=(5*len(y_axis), 3.5))\n",
    "    \n",
    "    for k,v in df_filter.items():        \n",
    "        mask = df[k].apply(lambda x: x == v)\n",
    "        if isinstance(v, list):\n",
    "            df = df[mask]\n",
    "    \n",
    "    baseline_df = df\n",
    "    \n",
    "    for k,v in baseline_filter.items():\n",
    "        #assert not isinstance(v, list)\n",
    "        #baseline_df = baseline_df[baseline_df[k]==v]\n",
    "        mask = baseline_df[k].apply(lambda x: x == v)\n",
    "        baseline_df = baseline_df[mask]\n",
    "        baseline_df = df[mask]\n",
    "        #print(mask)\n",
    "        #if isinstance(v, list):\n",
    "            #df[k].eq(v)\n",
    "            #df[k] = df[k].apply(lambda x: [0, 0, 0] if len(x) != 3 else x)\n",
    "            #df = df[df[k].eq(v)]\n",
    "            \n",
    "        #df = df[df[k]!=v]\n",
    "    \n",
    "    diff_df = pd.merge(df, baseline_df, left_on=x_axis,right_on=x_axis)\n",
    "    \n",
    "    if normalize_x_axis:\n",
    "        if x_axis == 'cluster_size':\n",
    "            # Hardcoded in Philly trace, precomputed ahead of time\n",
    "            if df['dataset'].iloc[0] == 'philly':\n",
    "                total_job_volume = 1155998.77277777\n",
    "                job_makespan = 2559.3205555555555\n",
    "            elif df['dataset'].iloc[0] == 'helios':\n",
    "                total_job_volume = 1853756.1241666232\n",
    "                job_makespan = 4651.911388888889\n",
    "            diff_df['norm_system_utilization'] = total_job_volume/(job_makespan*diff_df['cluster_size']*df['gpus_per_node'].iloc[0])\n",
    "            x_axis = 'norm_system_utilization'\n",
    "        elif x_axis == 'arrival_rate':\n",
    "            avg_job_volume_rate = diff_df['arrival_rate'] * np.mean(df['num_gpus'].iloc[0]* df['runtime'].iloc[0])\n",
    "            #print(avg_job_volume_rate)\n",
    "            #print(df['cluster_size'].iloc[0])\n",
    "            #print(df['gpus_per_node'].iloc[0])\n",
    "            #print(df['cluster_size'].iloc[0]*df['gpus_per_node'].iloc[0])\n",
    "            diff_df['norm_system_utilization'] = avg_job_volume_rate/(df['cluster_size'].iloc[0]*df['gpus_per_node'].iloc[0])\n",
    "            x_axis = 'norm_system_utilization'\n",
    "        \n",
    "    #return baseline_df\n",
    "    return diff_df\n",
    "\n",
    "def simulator_plotting_fn(df, \n",
    "                          x_axis: str,\n",
    "                          y_axis: list = ['cost_mult', 'avg_jct'],\n",
    "                          df_filter: dict = {},\n",
    "                          baseline_filter: dict={'waiting_policy': 'zero',},\n",
    "                          groupby_values=['waiting_policy', 'waiting_factor'],\n",
    "                          normalize_x_axis=False,\n",
    "                          fig_ratio=(5, 3.5)):\n",
    "    if isinstance(y_axis, str):\n",
    "        y_axis = [y_axis]\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(y_axis), figsize=(fig_ratio[0]*len(y_axis), fig_ratio[1]))\n",
    "    \n",
    "    if len(y_axis) == 1: \n",
    "        if not isinstance(axs, list):\n",
    "            axs = [axs]\n",
    "    print(axs)\n",
    "    #time.sleep(1000)\n",
    "    \n",
    "    '''\n",
    "    for k,v in df_filter.items():\n",
    "        if isinstance(v, list):\n",
    "            df = df[df[k]==v]\n",
    "    '''\n",
    "\n",
    "    for k,v in df_filter.items():        \n",
    "        mask = df[k].apply(lambda x: x == v)\n",
    "        if isinstance(v, list):\n",
    "            df = df[mask]\n",
    "       \n",
    "    baseline_df = df\n",
    "    \n",
    "    for k,v in baseline_filter.items():\n",
    "        assert not isinstance(v, list)\n",
    "        baseline_df = baseline_df[baseline_df[k]==v]\n",
    "        #df = df[df[k]!=v]\n",
    "        \n",
    "    diff_df = pd.merge(df, baseline_df, left_on=x_axis,right_on=x_axis)\n",
    "    \n",
    "    if normalize_x_axis:\n",
    "        if x_axis == 'cluster_size':\n",
    "            # Hardcoded in Philly trace, precomputed ahead of time\n",
    "            if df['dataset'].iloc[0] == 'philly':\n",
    "                total_job_volume = 1155998.77277777\n",
    "                job_makespan = 2559.3205555555555\n",
    "            elif df['dataset'].iloc[0] == 'helios':\n",
    "                total_job_volume = 1853756.1241666232\n",
    "                job_makespan = 4651.911388888889\n",
    "            diff_df['norm_system_utilization'] = total_job_volume/(job_makespan*diff_df['cluster_size']*df['gpus_per_node'].iloc[0])\n",
    "            x_axis = 'norm_system_utilization'\n",
    "        elif x_axis == 'arrival_rate':\n",
    "            # Volume rate => runtime must be in either hours or seconds \n",
    "            '''\n",
    "            Verification: \n",
    "            - Arrival_Rate = VERIFIED ~ error \n",
    "            - Num_gpus = Verified\n",
    "            - Runtime = Verified \n",
    "            '''\n",
    "            avg_job_volume_rate = diff_df['arrival_rate'] * np.mean(df['num_gpus'].iloc[0]* df['runtime'].iloc[0])\n",
    "            df['z_mean_runtime'] = np.mean(df['num_gpus'].iloc[0]* df['runtime'].iloc[0])\n",
    "            df['z_cluster_nodes'] = (df['cluster_size'].iloc[0]*df['gpus_per_node'].iloc[0])\n",
    "            #print(avg_job_volume_rate)\n",
    "            #print(df['cluster_size'].iloc[0])\n",
    "            #print(df['gpus_per_node'].iloc[0])\n",
    "            #print(df['cluster_size'].iloc[0]*df['gpus_per_node'].iloc[0])\n",
    "            diff_df['norm_system_utilization'] = avg_job_volume_rate/(df['cluster_size'].iloc[0]*df['gpus_per_node'].iloc[0])\n",
    "            x_axis = 'norm_system_utilization'\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    def cost_multiplier(row):\n",
    "        baseline_cost = row['total_cloud_cost_y']\n",
    "        cost = row['total_cloud_cost_x']\n",
    "        if baseline_cost == 0 and cost==0:\n",
    "            return 0\n",
    "        elif baseline_cost <=10000:\n",
    "            # Small cloud cost for No wait\n",
    "            # Savings over small cloud cost is negligible for organizations.\n",
    "            return 0\n",
    "        elif baseline_cost == 0 and cost>0:\n",
    "            return 100\n",
    "        return 100* (1 - (cost/baseline_cost))\n",
    "    \n",
    "    def cost_difference(row):\n",
    "        baseline_cost = row['total_cloud_cost_y']\n",
    "        cost = row['total_cloud_cost_x']\n",
    "        return baseline_cost - cost\n",
    "    \n",
    "    diff_df['cost_mult'] = diff_df.apply(cost_multiplier, axis=1)\n",
    "    diff_df['cost_diff'] = diff_df.apply(cost_difference, axis=1)\n",
    "    groupby_values = [f'{g}_x' for g in groupby_values]\n",
    "    mod_y_axis = [f'{y}_x' if y!='cost_mult' and y!='cost_diff' else y for y in y_axis]\n",
    "    import itertools\n",
    "    markers = itertools.cycle(('v', '^','.', 'o', '*',',', '+',)) \n",
    "    for idx, (label, grp) in enumerate(diff_df.groupby(groupby_values)):\n",
    "        marker = next(markers)\n",
    "#         if 'waiting_policy' in groupby_values[0]:\n",
    "#             label = [legend_dict[label[0]]] + list(label[1:])\n",
    "#             print(label)\n",
    "        for ax_idx, ax in enumerate(axs):           \n",
    "            grp.plot(x = x_axis, y = mod_y_axis[ax_idx],ylabel=label_dict[y_axis[ax_idx]], \\\n",
    "                     xlabel=label_dict[x_axis], marker=marker, ax = ax, label = label, legend=None)\n",
    "    \n",
    "    \n",
    "    for ax in axs:\n",
    "        set_plotting_setting(ax)\n",
    "    #axs[1].set_xlim(right=36, left=20)\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(lines, labels, ncol=len(labels), \\\n",
    "               bbox_to_anchor=(0, 0.92, 1, 0.2),loc='upper center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19a435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51834ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_df = get_default(jobs_df, x_axis='arrival_rate', y_axis=['avg_jct'], \\\n",
    "                           #df_filter={'cpu_dist': [0.2, 0.4, 0.4]}, \\\n",
    "                         #df_filter={'cpu_dist': [0.0, 0.5, 0.5]}, \\\n",
    "                         df_filter={'cpu_dist': [0.6, 0.25, 0.15]}, \\\n",
    "                          #df_filter={'wait_time': 1.0}, \\\n",
    "                      #baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 1.0}, \\\n",
    "                          #baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 0.0}, \\\n",
    "                         baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 0}, \\\n",
    "                           groupby_values=['waiting_policy'], normalize_x_axis=True)\n",
    "\n",
    "\n",
    "baseline_df = get_baseline(jobs_df, x_axis='arrival_rate', y_axis=['avg_jct'], \\\n",
    "                           #df_filter={'cpu_dist': [0.2, 0.4, 0.4]}, \\\n",
    "                           #df_filter={'cpu_dist': [0.0, 0.5, 0.5]}, \\\n",
    "                           df_filter={'cpu_dist': [0.6, 0.25, 0.15]},\n",
    "                      #baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 1.0}, \\\n",
    "                           baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 0}, \\\n",
    "                           groupby_values=['waiting_policy'], normalize_x_axis=True)\n",
    "\n",
    "\n",
    "merged_df = join_baseline(jobs_df, x_axis='arrival_rate', y_axis=['avg_jct', 'total_cloud_cost'], \\\n",
    "                          #baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 1.0}, \\\n",
    "                          baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 0}, \\\n",
    "                      groupby_values=['waiting_policy'], normalize_x_axis=True)\n",
    "default_df, baseline_df, merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a83ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddddb4b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dists = [[0.2, 0.4, 0.4], [0, 0.5, 0.5], [0, 0, 1]]\n",
    "dists = [[0.6, 0.25, 0.15]]#[[0, 0.5, 0.5]]\n",
    "scale = 4\n",
    "for dist in dists:#[[0.2, 0.4, 0.4], [0, 0.5, 0.5], [0, 0, 1]]:\n",
    "    #print(f'dist: {dist}')\n",
    "    simulator_plotting_fn(jobs_df, x_axis='arrival_rate', \\\n",
    "                          #y_axis=['total_cloud_cost'], \n",
    "                          y_axis=['avg_jct', 'cluster_utilization'],\\\n",
    "                          #y_axis=['avg_jct'],\\\n",
    "                          #'avg_jct', 'total_cloud_cost'], \\\n",
    "                          df_filter={'cpu_dist': dist}, \n",
    "                          #baseline_filter= {'wait_time': 1.0}, \\\n",
    "                          #baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 0.0}, \\\n",
    "                          baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 0.0}, \\\n",
    "                          #baseline_filter= {'waiting_policy': 'fifo_wait', 'wait_time': 1.0}, \\\n",
    "                          #groupby_values=['waiting_policy', 'wait_time'], normalize_x_axis=True, \\\n",
    "                          groupby_values=['wait_time'], normalize_x_axis=True, \\\n",
    "                          fig_ratio=(5*scale, 3.5*scale))\n",
    "    \n",
    "# TODO: Add spec to increase plot size\n",
    "#simulator_plotting_fn(t_metrics_df, x_axis='arrival_rate', y_axis=['avg_jct'], \\\n",
    "#                      groupby_values=['waiting_policy'], normalize_x_axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = copy.deepcopy(sampled_jobs.DEFAULT_HYPERPARAMETERS)\n",
    "\n",
    "hyp['arrival_rate'] = 10\n",
    "hyp['batch_time'] = 600\n",
    "hyp['total_jobs'] = 5\n",
    "hyp['mean_duration'] = 60\n",
    "arrived_jobs, arrivals = sampled_jobs.generate_jobs(hyp)\n",
    "\n",
    "#print(arrived_jobs)\n",
    "#print(arrivals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af943386",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETER\n",
    "'''\n",
    "Vary arrival rate\n",
    "    - CPU jobs for Philly/Helios\n",
    "        \n",
    "- Mimic GPU jobs ~ with CPU\n",
    "    - vary # jobs per minute \n",
    "    - test contant waiting for baseline\n",
    "    - 0(Baseline), 0.25, 0.5, 1 ratio of runtime for waiting policy \n",
    "        - \n",
    "    \n",
    "- Plot Cluster Utilization \n",
    "- Plot Cloud Cost \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d548c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
